---
layout: post
title: "Some questions while learning Machine Learning"
date: 2016-09-30
excerpt: ""
tags: [Computer Science, Machine Learning]
comments: true
---

### Some questions while learning Machine Learning 

----------
今儿看机器学习的公开课，想到几个问题，正好之前看深度学习的时候也想过的，随便一记，自己的整理、直观理解和一点idea，可能用词很不规范，主要目的是备忘，省的以后又想这些问题。同时有些问题应当有人想过，以后接触到的时候也可以回头来比对，慢慢了解吧。

1. 
	#### 关于sigmoid用于二分类：从线性回归到逻辑回归
	* 简单的说：线性函数很难拟合到0，1的值域分布，而sigmoid可以完美拟合，同时函数取值具有“可能性”的语义
	* 从另一个角度，如果根据线性函数的取值是否大于某个阀值如0.5来做出分类决策，直观理解，函数值越远大于0.5，即趋向于正无穷，分类越倾向于1。所以应当使得训练数据的函数取值在正或负无穷，这当然不好操作，那么可以让其尽量大或尽量小，同时设置合理的距离函数，或者loss function, quadratic loss function 显然不合适，应当选择一个边际效应递减的loss function。很容易想到一个解决方案就是值域映射，将(-inf, inf) mapping to  [0,1], z to sigmoid(z)。以及与之配合的log-likelihood loss function。
    * 其实，如果把中间值z作为output的话，是不是还可以理解为线性回归，只是loss function相对复杂了。以及ground-truth y 应当取值为 -inf or inf。
    * 具有同样mapping功能的函数不只有sigmoid，比如tanh。但关于是否sigmoid具有什么不可替代的优良特性，tanh等函数是否有与之匹配的loss function，我还没有考虑。

2. 
	#### 关于regularization 约束参数二范数
    * 约束高次方参数，使hypothesis更加趋近低次方的表达，即更简约而普适的表达；不过就算低次方的参数一起约束，也说得通，就是更趋近于0呗。
    * 二范数倾向于参数低方差，使参数分布更加均衡，一来避免了对个别参数的依赖和过度响应 带来的学习波动，二来限制了参数多样性带来的多种拟合方案 以防止不恰当的拟合 过拟合
    * 我认为对参数的惩罚与其维度正相关会带来更好的正规化效果

3. 
	#### 关于梯度下降
    * 方向
        - 如果不选梯度方向？梯度方向是在dx大小不变的约束下dy最大的方向，但是并没有dx不变的限制。
          - 在每个方向都达到同样的下降高度？
            - 不存在每个方向的概念，这里有轴位选取的问题，没细想
          - 模拟小球滑落的话，在某个点上，梯度方向为合力、即加速度方向，而速度取决于之前的运行状态。［其实相当于第三条中学习率的调节反映在各个方向中的结果，反复的震荡对应速度降低的效果，而一马平川则速度增大］
            - 通过调节重力加速度来控制收敛速度
    * 梯度方向的控制
        - 假定选定梯度方向，通过feature scaling 控制梯度方向为全局最小值的方向。 控制每个变量规模在－1～1
    * 步伐、速度的选取
        - 学习率自反馈调节，如果发现震荡则调低学习率，如果稳定在某个方向很长时间则增大学习率。
        - 确保每个迭代都在降低loss
        - 固定的dy？为什么f'(x)越大选择越大的dx?
          - 固定dy没什么意义，dx与f'(x)成正比可能是考虑到极小值附近减小震荡尽快收敛。
    

3. 
	#### 关于学习缓慢
    * 通过loss function的选择，看起来并不能解决前几层的学习缓慢问题啊？

----------

